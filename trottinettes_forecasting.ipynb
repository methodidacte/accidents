{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/forecasting-orange-juice-sales/auto-ml-forecasting-orange-juice-sales.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Orange Juice Sales Forecasting**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Predict](#Predict)\n",
    "1. [Operationalize](#Operationalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example, we use AutoML to train, select, and operationalize a time-series forecasting model for multiple time-series.\n",
    "\n",
    "Make sure you have executed the [configuration notebook](../../../configuration.ipynb) before running this notebook.\n",
    "\n",
    "The examples in the follow code samples use the University of Chicago's Dominick's Finer Foods dataset to forecast orange juice sales. Dominick's was a grocery chain in the Chicago metropolitan area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0918 12:20:15.691082 140082692052736 deprecation_wrapper.py:119] From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/automl/core/_vendor/automl/client/core/common/tf_wrappers.py:36: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0918 12:20:15.692417 140082692052736 deprecation_wrapper.py:119] From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/automl/core/_vendor/automl/client/core/common/tf_wrappers.py:36: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SDK version</th>\n",
       "      <td>1.0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subscription ID</th>\n",
       "      <td>f80606e5-788f-4dc3-a9ea-2eb9a7836082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Workspace</th>\n",
       "      <td>eacbmlservicews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resource Group</th>\n",
       "      <td>adlsgen2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>westus2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Project Directory</th>\n",
       "      <td>./accidents/trottinettes-forecasting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run History Name</th>\n",
       "      <td>trottinettes-forecasting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       \n",
       "SDK version        1.0.53                              \n",
       "Subscription ID    f80606e5-788f-4dc3-a9ea-2eb9a7836082\n",
       "Workspace          eacbmlservicews                     \n",
       "Resource Group     adlsgen2                            \n",
       "Location           westus2                             \n",
       "Project Directory  ./accidents/trottinettes-forecasting\n",
       "Run History Name   trottinettes-forecasting            "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'trottinettes-forecasting'\n",
    "# project folder\n",
    "project_folder = './accidents/trottinettes-forecasting'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "You are now ready to load the historical orange juice sales data. We will load the CSV file into a plain pandas DataFrame; the time column in the CSV is called _WeekStarting_, so it will be specially parsed into the datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accidents</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accidents       date         name\n",
       "0  13        2010-01-01  trottinette\n",
       "1  11        2010-02-01  trottinette\n",
       "2  19        2010-03-01  trottinette\n",
       "3  16        2010-04-01  trottinette\n",
       "4  22        2010-05-01  trottinette"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_column_name = 'date'\n",
    "data = pd.read_csv(\"trottinettes.tsv\", sep='\\t', parse_dates=[time_column_name])\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the DataFrame holds a quantity of weekly sales for an OJ brand at a single store. The data also includes the sales price, a flag indicating if the OJ brand was advertised in the store that week, and some customer demographic information based on the store location. For historical reasons, the data also include the logarithm of the sales quantity. The Dominick's grocery data is commonly used to illustrate econometric modeling techniques where logarithms of quantities are generally preferred.    \n",
    "\n",
    "The task is now to build a time-series model for the _Quantity_ column. It is important to note that this dataset is comprised of many individual time-series - one for each unique combination of _Store_ and _Brand_. To distinguish the individual time-series, we thus define the **grain** - the columns whose values determine the boundaries between time-series: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains 1 individual time-series.\n"
     ]
    }
   ],
   "source": [
    "grain_column_names = ['name']\n",
    "nseries = data.groupby(grain_column_names).ngroups\n",
    "print('Data contains {0} individual time-series.'.format(nseries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "We now split the data into a training and a testing set for later forecast evaluation. The test set will contain the final 20 weeks of observed sales for each time-series. The splits should be stratified by series, so we use a group-by statement on the grain columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_periods = 12\n",
    "\n",
    "def split_last_n_by_grain(df, n):\n",
    "    \"\"\"Group df by grain and split on last n rows for each group.\"\"\"\n",
    "    df_grouped = (df.sort_values(time_column_name) # Sort by ascending time\n",
    "                  .groupby(grain_column_names, group_keys=False))\n",
    "    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
    "    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
    "    return df_head, df_tail\n",
    "\n",
    "X_train, X_test = split_last_n_by_grain(data, n_test_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 3)\n",
      "(12, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "For forecasting tasks, AutoML uses pre-processing and estimation steps that are specific to time-series. AutoML will undertake the following pre-processing steps:\n",
    "* Detect time-series sample frequency (e.g. hourly, daily, weekly) and create new records for absent time points to make the series regular. A regular time series has a well-defined frequency and has a value at every sample point in a contiguous time span \n",
    "* Impute missing values in the target (via forward-fill) and feature columns (using median column values) \n",
    "* Create grain-based features to enable fixed effects across different series\n",
    "* Create time-based features to assist in learning seasonal patterns\n",
    "* Encode categorical variables to numeric quantities\n",
    "\n",
    "AutoML will currently train a single, regression-type model across **all** time-series in a given training set. This allows the model to generalize across related series.\n",
    "\n",
    "You are almost ready to start an AutoML training job. First, we need to separate the target column from the rest of the DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'accidents'\n",
    "y_train = X_train.pop(target_column_name).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The AutoMLConfig object defines the settings and data for an AutoML training job. Here, we set necessary inputs like the task type, the number of AutoML iterations to try, the training data, and cross-validation parameters. \n",
    "\n",
    "For forecasting tasks, there are some additional parameters that can be set: the name of the column holding the date/time, the grain column names, and the maximum forecast horizon. A time column is required for forecasting, while the grain is optional. If a grain is not given, AutoML assumes that the whole dataset is a single time-series. We also pass a list of columns to drop prior to modeling. The _logQuantity_ column is completely correlated with the target quantity, so it must be removed to prevent a target leak.\n",
    "\n",
    "The forecast horizon is given in units of the time-series frequency; for instance, the OJ series frequency is weekly, so a horizon of 20 means that a trained model will estimate sales up-to 20 weeks beyond the latest date in the training data for each series. In this example, we set the maximum horizon to the number of samples per series in the test set (n_test_periods). Generally, the value of this parameter will be dictated by business needs. For example, a demand planning organizaion that needs to estimate the next month of sales would set the horizon accordingly. Please see the [energy_demand notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand) for more discussion of forecast horizon.\n",
    "\n",
    "Finally, a note about the cross-validation (CV) procedure for time-series data. AutoML uses out-of-sample error estimates to select a best pipeline/model, so it is important that the CV fold splitting is done correctly. Time-series can violate the basic statistical assumptions of the canonical K-Fold CV strategy, so AutoML implements a [rolling origin validation](https://robjhyndman.com/hyndsight/tscv/) procedure to create CV folds for time-series data. To use this procedure, you just need to specify the desired number of CV folds in the AutoMLConfig object. It is also possible to bypass CV and use your own validation set by setting the *X_valid* and *y_valid* parameters of AutoMLConfig.\n",
    "\n",
    "Here is a summary of AutoMLConfig parameters used for training the OJ model:\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
    "|**iterations**|Number of iterations. In each iteration, Auto ML trains a specific pipeline on the given data|\n",
    "|**X**|Training matrix of features as a pandas DataFrame, shape = [n_training_samples, n_features]|\n",
    "|**y**|Target values as a numpy.ndarray, shape = [n_training_samples, ]|\n",
    "|**n_cross_validations**|Number of cross-validation folds to use for model/pipeline selection|\n",
    "|**enable_voting_ensemble**|Allow AutoML to create a Voting ensemble of the best performing models\n",
    "|**enable_stack_ensemble**|Allow AutoML to create a Stack ensemble of the best performing models\n",
    "|**debug_log**|Log file path for writing debugging information\n",
    "|**path**|Relative path to the project folder.  AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder.|\n",
    "|**time_column_name**|Name of the datetime column in the input data|\n",
    "|**grain_column_names**|Name(s) of the columns defining individual series in the input data|\n",
    "|**drop_column_names**|Name(s) of columns to drop prior to modeling|\n",
    "|**max_horizon**|Maximum desired forecast horizon in units of time-series frequency|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_settings = {\n",
    "    'time_column_name': time_column_name,\n",
    "    'grain_column_names': grain_column_names,\n",
    "    'drop_column_names': [],\n",
    "    'max_horizon': n_test_periods\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl_trottinettes_errors.log',\n",
    "                             primary_metric='normalized_mean_absolute_error',\n",
    "                             iterations=10,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             n_cross_validations=2,\n",
    "                             enable_voting_ensemble=False,\n",
    "                             enable_stack_ensemble=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,\n",
    "                             **time_series_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now submit a new training run. For local runs, the execution is synchronous. Depending on the data and number of iterations this operation may take several minutes.\n",
    "Information from each iteration will be printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local machine\n",
      "Parent Run ID: AutoML_707590aa-0240-4c61-808e-c715bea56b30\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating CV splits.\n",
      "Current status: DatasetFeaturization. Beginning to featurize the CV split.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the CV split.\n",
      "Current status: DatasetFeaturization. Beginning to featurize the CV split.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the CV split.\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:16       0.2001    0.2001\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:17       0.1992    0.1992\n",
      "         2   StandardScalerWrapper RandomForest             0:00:18       0.1317    0.1317\n",
      "         3   StandardScalerWrapper ExtremeRandomTrees       0:00:17       0.1089    0.1089\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:17       0.1788    0.1089\n",
      "         5   MinMaxScaler DecisionTree                      0:00:17       0.1868    0.1089\n",
      "         6   RobustScaler LassoLars                         0:00:17       0.2065    0.1089\n",
      "         7   MinMaxScaler RandomForest                      0:00:17       0.1465    0.1089\n",
      "         8   StandardScalerWrapper LassoLars                0:00:17       0.1918    0.1089\n",
      "         9   StandardScalerWrapper DecisionTree             0:00:17       0.1505    0.1089\n"
     ]
    }
   ],
   "source": [
    "local_run = experiment.submit(automl_config, show_output=True)\n",
    "# Duplicates in time and grain combinations\n",
    "# The data provided is insufficient for training : The data points should have at least 43 for a valid training with cv 2, max_horizon 20, lags [0] and rolling window size 0. The current dataset has only 30 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model\n",
    "Each run within an Experiment stores serialized (i.e. pickled) pipelines from the AutoML iterations. We can now retrieve the pipeline with the best performance on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_pipeline = local_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>trottinettes-forecasting</td><td>AutoML_707590aa-0240-4c61-808e-c715bea56b30_3</td><td></td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/f80606e5-788f-4dc3-a9ea-2eb9a7836082/resourceGroups/adlsgen2/providers/Microsoft.MachineLearningServices/workspaces/eacbmlservicews/experiments/trottinettes-forecasting/runs/AutoML_707590aa-0240-4c61-808e-c715bea56b30_3\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.Run?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: trottinettes-forecasting,\n",
       "Id: AutoML_707590aa-0240-4c61-808e-c715bea56b30_3,\n",
       "Type: None,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timeseriestransformer', TimeSeriesTransformer(logger=None,\n",
       "                        pipeline_type=<TimeSeriesPipelineType.FULL: 1>)),\n",
       " ('StandardScalerWrapper',\n",
       "  <automl.client.core.common.model_wrappers.StandardScalerWrapper at 0x7f675972ca90>),\n",
       " ('ExtraTreesRegressor',\n",
       "  ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
       "                      max_features=0.3, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=0.001953125,\n",
       "                      min_samples_split=0.00310675990983383,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "                      oob_score=False, random_state=None, verbose=0,\n",
       "                      warm_start=False))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "Now that we have retrieved the best pipeline/model, it can be used to make predictions on test data. First, we remove the target values from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = X_test.pop(target_column_name).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>trottinette</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date         name\n",
       "84 2017-01-01  trottinette\n",
       "85 2017-02-01  trottinette\n",
       "86 2017-03-01  trottinette\n",
       "87 2017-04-01  trottinette\n",
       "88 2017-05-01  trottinette\n",
       "89 2017-06-01  trottinette\n",
       "90 2017-07-01  trottinette\n",
       "91 2017-08-01  trottinette\n",
       "92 2017-09-01  trottinette\n",
       "93 2017-10-01  trottinette\n",
       "94 2017-11-01  trottinette\n",
       "95 2017-12-01  trottinette"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 11, 24, 27, 21, 41, 27, 17, 30, 29, 23, 20])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce predictions on the test set, we need to know the feature values at all dates in the test set. This requirement is somewhat reasonable for the OJ sales data since the features mainly consist of price, which is usually set in advance, and customer demographics which are approximately constant for each store over the 20 week forecast horizon in the testing data. \n",
    "\n",
    "We will first create a query `y_query`, which is aligned index-for-index to `X_test`. This is a vector of target values where each `NaN` serves the function of the question mark to be replaced by forecast. Passing definite values in the `y` argument allows the `forecast` function to make predictions on data that does not immediately follow the train data which contains `y`. In each grain, the last time point where the model sees a definite value of `y` is that grain's _forecast origin_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ALL values in y_pred by NaN.\n",
    "# The forecast origin will be at the beginning of the first forecast period.\n",
    "# (Which is the same time as the end of the last training period.)\n",
    "y_query = y_test.copy().astype(np.float)\n",
    "y_query.fill(np.nan)\n",
    "# The featurized data, aligned to y, will also be returned.\n",
    "# This contains the assumptions that were made in the forecast\n",
    "# and helps align the forecast to the original data\n",
    "y_pred, X_trans = fitted_pipeline.forecast(X_test, y_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are used to scikit pipelines, perhaps you expected `predict(X_test)`. However, forecasting requires a more general interface that also supplies the past target `y` values. Please use `forecast(X,y)` as `predict(X)` is reserved for internal purposes on forecasting models.\n",
    "\n",
    "The [energy demand forecasting notebook](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/forecasting-energy-demand) demonstrates the use of the forecast function in more detail in the context of using lags and rolling window features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.2 , 17.55, 24.15, 18.2 , 19.  , 27.3 , 20.2 , 16.8 , 30.  ,\n",
       "       21.  , 12.9 , 19.9 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "To evaluate the accuracy of the forecast, we'll compare against the actual sales quantities for some select metrics, included the mean absolute percentage error (MAPE). \n",
    "\n",
    "It is a good practice to always align the output explicitly to the input, as the count and order of the rows may have changed during transformations that span multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_outputs(y_predicted, X_trans, X_test, y_test, predicted_column_name = 'predicted'):\n",
    "    \"\"\"\n",
    "    Demonstrates how to get the output aligned to the inputs\n",
    "    using pandas indexes. Helps understand what happened if\n",
    "    the output's shape differs from the input shape, or if\n",
    "    the data got re-sorted by time and grain during forecasting.\n",
    "    \n",
    "    Typical causes of misalignment are:\n",
    "    * we predicted some periods that were missing in actuals -> drop from eval\n",
    "    * model was asked to predict past max_horizon -> increase max horizon\n",
    "    * data at start of X_test was needed for lags -> provide previous periods in y\n",
    "    \"\"\"\n",
    "    \n",
    "    df_fcst = pd.DataFrame({predicted_column_name : y_predicted})\n",
    "    # y and X outputs are aligned by forecast() function contract\n",
    "    df_fcst.index = X_trans.index\n",
    "    \n",
    "    # align original X_test to y_test    \n",
    "    X_test_full = X_test.copy()\n",
    "    X_test_full[target_column_name] = y_test\n",
    "\n",
    "    # X_test_full's index does not include origin, so reset for merge\n",
    "    df_fcst.reset_index(inplace=True)\n",
    "    X_test_full = X_test_full.reset_index().drop(columns='index')\n",
    "    together = df_fcst.merge(X_test_full, how='right')\n",
    "    \n",
    "    # drop rows where prediction or actuals are nan \n",
    "    # happens because of missing actuals \n",
    "    # or at edges of time due to lags/rolling windows\n",
    "    clean = together[together[[target_column_name, predicted_column_name]].notnull().all(axis=1)]\n",
    "    return(clean)\n",
    "\n",
    "df_all = align_outputs(y_pred, X_trans, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(actual, pred):\n",
    "    \"\"\"\n",
    "    Calculate mean absolute percentage error.\n",
    "    Remove NA and values where actual is close to zero\n",
    "    \"\"\"\n",
    "    not_na = ~(np.isnan(actual) | np.isnan(pred))\n",
    "    not_zero = ~np.isclose(actual, 0.0)\n",
    "    actual_safe = actual[not_na & not_zero]\n",
    "    pred_safe = pred[not_na & not_zero]\n",
    "    APE = 100*np.abs((actual_safe - pred_safe)/actual_safe)\n",
    "    return np.mean(APE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple forecasting model\n",
      "[Test Data] \n",
      "Root Mean squared error: 6.61\n",
      "mean_absolute_error score: 4.77\n",
      "MAPE: 19.86\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Simple forecasting model\")\n",
    "rmse = np.sqrt(mean_squared_error(df_all[target_column_name], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_column_name], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.2f' % mae)\n",
    "print('MAPE: %.2f' % MAPE(df_all[target_column_name], df_all['predicted']))\n",
    "\n",
    "# Plot outputs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_test = plt.scatter(y_test, y_test, color='g')\n",
    "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Operationalization_ means getting the model into the cloud so that other can run it after you close the notebook. We will create a docker running on Azure Container Instances with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model AutoML707590aa0best\n",
      "AutoML707590aa0best\n"
     ]
    }
   ],
   "source": [
    "description = 'AutoML trottinettes forecaster'\n",
    "tags = None\n",
    "model = local_run.register_model(description = description, tags = tags)\n",
    "\n",
    "print(local_run.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the scoring script\n",
    "\n",
    "Serializing and deserializing complex data frames may be tricky. We first develop the `run()` function of the scoring script locally, then write it into a scoring script. It is much easier to debug any quirks of the scoring function without crossing two compute environments. For this exercise, we handle a common quirk of how pandas dataframes serialize time stamp values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where we test the run function of the scoring script interactively\n",
    "# before putting it in the scoring script\n",
    "\n",
    "timestamp_columns = ['date']\n",
    "\n",
    "def run(rawdata, test_model = None):\n",
    "    \"\"\"\n",
    "    Intended to process 'rawdata' string produced by\n",
    "    \n",
    "    {'X': X_test.to_json(), y' : y_test.to_json()}\n",
    "    \n",
    "    Don't convert the X payload to numpy.array, use it as pandas.DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # unpack the data frame with timestamp        \n",
    "        rawobj = json.loads(rawdata)                    # rawobj is now a dict of strings        \n",
    "        X_pred = pd.read_json(rawobj['X'], convert_dates=False)   # load the pandas DF from a json string\n",
    "        for col in timestamp_columns:                             # fix timestamps\n",
    "            X_pred[col] = pd.to_datetime(X_pred[col], unit='ms') \n",
    "        \n",
    "        y_pred = np.array(rawobj['y'])                    # reconstitute numpy array from serialized list\n",
    "        \n",
    "        if test_model is None:\n",
    "            result = model.forecast(X_pred, y_pred)       # use the global model from init function\n",
    "        else:\n",
    "            result = test_model.forecast(X_pred, y_pred)  # use the model on which we are testing\n",
    "        \n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "    \n",
    "    forecast_as_list = result[0].tolist()\n",
    "    index_as_df = result[1].index.to_frame().reset_index(drop=True)\n",
    "    \n",
    "    return json.dumps({\"forecast\": forecast_as_list,   # return the minimum over the wire: \n",
    "                       \"index\": index_as_df.to_json()  # no forecast and its featurized values\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>18.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>17.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>24.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>18.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date         name  forecast\n",
       "0  2017-01-01  trottinette 18.20    \n",
       "1  2017-02-01  trottinette 17.55    \n",
       "10 2017-11-01  trottinette 24.15    \n",
       "11 2017-12-01  trottinette 18.20    \n",
       "2  2017-03-01  trottinette 19.00    "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the run function here before putting in the scoring script\n",
    "import json\n",
    "\n",
    "test_sample = json.dumps({'X': X_test.to_json(), 'y' : y_query.tolist()})\n",
    "response = run(test_sample, fitted_pipeline)\n",
    "\n",
    "# unpack the response, dealing with the timestamp serialization again\n",
    "res_dict = json.loads(response)\n",
    "y_fcst_all = pd.read_json(res_dict['index'])\n",
    "y_fcst_all[time_column_name] = pd.to_datetime(y_fcst_all[time_column_name], unit = 'ms')\n",
    "y_fcst_all['forecast'] = res_dict['forecast']\n",
    "y_fcst_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the function works locally in the notebook, let's write it down into the scoring script. The scoring script is authored by the data scientist. Adjust it to taste, adding inputs, outputs and processing as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score_fcast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score_fcast.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import azureml.train.automl\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path(model_name = '<<modelid>>') # this name is model.id of model that we want to deploy\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "timestamp_columns = ['date']\n",
    "\n",
    "def run(rawdata, test_model = None):\n",
    "    \"\"\"\n",
    "    Intended to process 'rawdata' string produced by\n",
    "    \n",
    "    {'X': X_test.to_json(), y' : y_test.to_json()}\n",
    "    \n",
    "    Don't convert the X payload to numpy.array, use it as pandas.DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # unpack the data frame with timestamp        \n",
    "        rawobj = json.loads(rawdata)                    # rawobj is now a dict of strings        \n",
    "        X_pred = pd.read_json(rawobj['X'], convert_dates=False)   # load the pandas DF from a json string\n",
    "        for col in timestamp_columns:                             # fix timestamps\n",
    "            X_pred[col] = pd.to_datetime(X_pred[col], unit='ms') \n",
    "        \n",
    "        y_pred = np.array(rawobj['y'])                    # reconstitute numpy array from serialized list\n",
    "        \n",
    "        if test_model is None:\n",
    "            result = model.forecast(X_pred, y_pred)       # use the global model from init function\n",
    "        else:\n",
    "            result = test_model.forecast(X_pred, y_pred)  # use the model on which we are testing\n",
    "        \n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "    \n",
    "    # prepare to send over wire as json\n",
    "    forecast_as_list = result[0].tolist()\n",
    "    index_as_df = result[1].index.to_frame().reset_index(drop=True)\n",
    "    \n",
    "    return json.dumps({\"forecast\": forecast_as_list,   # return the minimum over the wire: \n",
    "                       \"index\": index_as_df.to_json()  # no forecast and its featurized values\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "ml_run = AutoMLRun(experiment = experiment, run_id = local_run.id)\n",
    "best_iteration = int(str.split(best_run.id,'_')[-1])      # the iteration number is a postfix of the run ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml-train-automl\t1.0.53\n",
      "azureml-core\t1.0.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fcast_env.yml'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best model's dependencies and write them into this file\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_env_file_name = 'fcast_env.yml'\n",
    "\n",
    "dependencies = ml_run.get_run_sdk_dependencies(iteration = best_iteration)\n",
    "for p in ['azureml-train-automl', 'azureml-core']:\n",
    "    print('{}\\t{}'.format(p, dependencies[p]))\n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn'], pip_packages=['azureml-train-automl'])\n",
    "\n",
    "myenv.save_to_file('.', conda_env_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the script file name we wrote a few cells above\n",
    "script_file_name = 'score_fcast.py'\n",
    "\n",
    "# Substitute the actual version number in the environment file.\n",
    "# This is not strictly needed in this notebook because the model should have been generated using the current SDK version.\n",
    "# However, we include this in case this code is used on an experiment from a previous SDK version.\n",
    "\n",
    "with open(conda_env_file_name, 'r') as cefr:\n",
    "    content = cefr.read()\n",
    "\n",
    "with open(conda_env_file_name, 'w') as cefw:\n",
    "    cefw.write(content.replace(azureml.core.VERSION, dependencies['azureml-train-automl']))\n",
    "\n",
    "# Substitute the actual model id in the script file.\n",
    "\n",
    "with open(script_file_name, 'r') as cefr:\n",
    "    content = cefr.read()\n",
    "\n",
    "with open(script_file_name, 'w') as cefw:\n",
    "    cefw.write(content.replace('<<modelid>>', local_run.model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Container Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Running...................................................\n",
      "Succeeded\n",
      "Image creation operation finished for image automl-fcast-image:1, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(runtime= \"python\",\n",
    "                                 execution_script = script_file_name,\n",
    "                                 conda_file = conda_env_file_name,\n",
    "                                 tags = {'type': \"automl-forecasting\"},\n",
    "                                 description = \"Image for automl forecasting sample\")\n",
    "\n",
    "image = Image.create(name = \"automl-fcast-image\",\n",
    "                     # this is the model object \n",
    "                     models = [model],\n",
    "                     image_config = image_config, \n",
    "                     workspace = ws)\n",
    "\n",
    "image.wait_for_creation(show_output = True)\n",
    "\n",
    "if image.creation_state == 'Failed':\n",
    "    print(\"Image build log at: \" + image.image_build_log_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Image as a Web Service on Azure Container Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 2, \n",
    "                                               tags = {'type': \"automl-forecasting\"},\n",
    "                                               description = \"Automl forecasting sample service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-forecast-01\n",
      "Creating service\n",
      "Running.....................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = 'automl-forecast-01'\n",
    "print(aci_service_name)\n",
    "\n",
    "aci_service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
    "                                           image = image,\n",
    "                                           name = aci_service_name,\n",
    "                                           workspace = ws)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we send the data to the service serialized into a json string\n",
    "test_sample = json.dumps({'X':X_test.to_json(), 'y' : y_query.tolist()})\n",
    "response = aci_service.run(input_data = test_sample)\n",
    "\n",
    "# translate from networkese to datascientese\n",
    "try: \n",
    "    res_dict = json.loads(response)\n",
    "    y_fcst_all = pd.read_json(res_dict['index'])\n",
    "    y_fcst_all[time_column_name] = pd.to_datetime(y_fcst_all[time_column_name], unit = 'ms')\n",
    "    y_fcst_all['forecast'] = res_dict['forecast']    \n",
    "except:\n",
    "    print(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>18.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>17.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>24.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>18.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>27.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>20.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>16.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>12.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>trottinette</td>\n",
       "      <td>19.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date         name  forecast\n",
       "0  2017-01-01  trottinette 18.20    \n",
       "1  2017-02-01  trottinette 17.55    \n",
       "10 2017-11-01  trottinette 24.15    \n",
       "11 2017-12-01  trottinette 18.20    \n",
       "2  2017-03-01  trottinette 19.00    \n",
       "3  2017-04-01  trottinette 27.30    \n",
       "4  2017-05-01  trottinette 20.20    \n",
       "5  2017-06-01  trottinette 16.80    \n",
       "6  2017-07-01  trottinette 30.00    \n",
       "7  2017-08-01  trottinette 21.00    \n",
       "8  2017-09-01  trottinette 12.90    \n",
       "9  2017-10-01  trottinette 19.90    "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fcst_all.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the web service if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serv = Webservice(ws, 'automl-forecast-01')\n",
    "# serv.delete()     # don't do it accidentally"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "erwright"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
